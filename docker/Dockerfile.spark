FROM apache/spark:3.5.3

USER root

# Install Python dependencies
# Install Python dependencies
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir --upgrade pip

# Install core data stack
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 \
    delta-spark==3.0.0 \
    pandas==2.0.3 \
    numpy==1.24.4 \
    scikit-learn==1.3.1

# Install web and API stack
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 \
    streamlit==1.28.0 \
    fastapi \
    uvicorn \
    plotly

# Install ML Ops and Utilities individually to handle network instability
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 mlflow==2.8.1
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 psycopg2-binary
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 kafka-python
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 pyjwt loguru boto3 python-dotenv
RUN pip install --no-cache-dir --default-timeout=1000 --retries 10 great-expectations==0.17.15

# Download Delta Lake and S3 connectors
ENV SPARK_PACKAGES="io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"

# Precaching jars
RUN /opt/spark/bin/spark-shell --packages $SPARK_PACKAGES --eval "sys.exit()"

WORKDIR /app
